{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Genre Classification\n",
    "In this notebook I tried to learn the basic concepts of neural networks and use it to classify the music files in dataset. Majorly this notebook can be divided into 3 parts:\n",
    "\n",
    "   1) Using ANN \n",
    "\n",
    "   2) Tackling overfitting with ANN\n",
    "\n",
    "   3) Using CNN\n",
    "\n",
    "\n",
    "Also to read the dataset I have used librosa library which only read files <1Mb and one file is greater than the size giving error due to which I have ignored it. The dataset contains the following genres, the keys being the prediction targets\n",
    "    \n",
    "    0: \"disco\",\n",
    "    1: \"metal\",\n",
    "    2: \"reggae\",\n",
    "    3: \"blues\",\n",
    "    4: \"rock\",\n",
    "    5: \"classical\",\n",
    "    6: \"jazz\",\n",
    "    7: \"hiphop\",\n",
    "    8: \"country\",\n",
    "    9: \"pop\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "01a50213-fe5d-431b-9ade-28c438b3bceb",
    "_uuid": "f32a1031-9332-42b6-8335-dee2b86310a3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import math\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e1ba8988-65f9-4697-a731-e71b25dd171b",
    "_uuid": "d401d5fb-ea22-4562-bc1b-e882f59016ce",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = r\"Data/genres_original\"\n",
    "json_path = r\"data.json\"\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f0f0a6db-b7e9-468f-890f-36312fe994e7",
    "_uuid": "c44ee71f-62e4-4100-8e2b-d2a84b01cf8e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def save_mfcc(dataset_path, json_path, n_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
    "    data = {\n",
    "        \"mapping\": [],\n",
    "        \"mfcc\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "    samples_ps = int(SAMPLES_PER_TRACK/num_segments)\n",
    "    expected_vects_ps = math.ceil(samples_ps/hop_length)\n",
    "    \n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        if dirpath is not dataset_path:\n",
    "            dirpath_comp = dirpath.split(\"/\")\n",
    "            semantic_label = dirpath_comp[-1]\n",
    "            data[\"mapping\"].append(semantic_label)\n",
    "            print(f\"Processing: {semantic_label}\")\n",
    "            \n",
    "            for f in filenames:\n",
    "                if f == \"jazz.00054.wav\":  # Check if this condition is necessary\n",
    "                    continue\n",
    "                else:\n",
    "                    file_path = os.path.join(dirpath, f)\n",
    "                    print(f\"Processing file: {file_path}\")\n",
    "                    \n",
    "                    signal, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "                    print(f\"Loaded signal with sampling rate: {sr}\")\n",
    "                    \n",
    "                    for s in range(num_segments):\n",
    "                        start_sample = samples_ps * s\n",
    "                        finish_sample = start_sample + samples_ps\n",
    "                        print(f\"Segment {s+1}: Start Sample: {start_sample}, Finish Sample: {finish_sample}\")\n",
    "                        \n",
    "                        mfcc = librosa.feature.mfcc(y=signal[start_sample:finish_sample],\n",
    "                            sr=sr,\n",
    "                            n_fft=n_fft,\n",
    "                            n_mfcc=n_mfcc,\n",
    "                            hop_length=hop_length)\n",
    "\n",
    "                        mfcc = mfcc.T\n",
    "\n",
    "                        if len(mfcc) == expected_vects_ps:\n",
    "                            data[\"mfcc\"].append(mfcc.tolist())\n",
    "                            data[\"labels\"].append(i-1)\n",
    "                            print(f\"{file_path}, segment: {s+1}\")\n",
    "\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mfcc(dataset_path,json_path,num_segments=10)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"Data/genres_original/blues/blues.0000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for i in range(2):\n",
    "    audio, sfreq = librosa.load(filepath+str(i)+\".wav\")\n",
    "    time = np.arange(0, len(audio))/sfreq\n",
    "    plt.plot(time,audio)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Sound Amplitude\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "This part uses the concepts of ANN with keras and sequential layers. I have also done splitting in the ratio 70:30\n",
    "\n",
    "The model is Sequential and architecture only has Flatten and the Dense layers available in keras for the basic ANN representation. As it is naive model we can expect it to be overfit. Info on the layers can be found [here](https://machinelearningknowledge.ai/different-types-of-keras-layers-explained-for-beginners/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_data(dataset_path):\n",
    "    with open(dataset_path,\"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Convert list to numpy arrays\n",
    "    inputs = np.array(data[\"mfcc\"])\n",
    "    targets = np.array(data[\"labels\"])    \n",
    "    \n",
    "    return inputs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,targets = load_data(r\"data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_train, input_test, target_train, target_test = train_test_split(inputs, targets, test_size=0.3)\n",
    "print(input_train.shape, target_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(inputs.shape[1],inputs.shape[2])))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "adam = optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam,\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam,\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "hist = model.fit(input_train, target_train,\n",
    "                 validation_data = (input_test,target_test),\n",
    "                 epochs = 50,\n",
    "                 batch_size = 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist):\n",
    "    plt.figure(figsize=(20,15))\n",
    "    fig, axs = plt.subplots(2)\n",
    "    # accuracy subplot\n",
    "    axs[0].plot(hist.history[\"accuracy\"], label=\"train accuracy\")\n",
    "    axs[0].plot(hist.history[\"val_accuracy\"], label=\"test accuracy\")    \n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy eval\")\n",
    "    \n",
    "    # Error subplot\n",
    "    axs[1].plot(hist.history[\"loss\"], label=\"train error\")\n",
    "    axs[1].plot(hist.history[\"val_loss\"], label=\"test error\")    \n",
    "    axs[1].set_ylabel(\"Error\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Error eval\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error, test_accuracy = model.evaluate(input_test, target_test, verbose=1)\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "This part deals with the overfitting of the previous model. We can deal with it by majorly doing the following process.\n",
    "- Making architecture less complicated \n",
    "- Using augmented data\n",
    "- Early stopping of training\n",
    "- Adding dropout layers\n",
    "- Regularization / Standardization  \n",
    "\n",
    "I have added the dropout layers and kernel_regularizers as compared to previous naive model giving the dropout probability as 30%\n",
    "Kernel_regularizers is one of the 3 type of regularizer used to impose penalties. More info can be found [here](https://medium.com/@robertjohn_15390/regularization-in-tensorflow-using-keras-api-48aba746ae21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(inputs.shape[1],inputs.shape[2])))\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer = keras.regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer = keras.regularizers.l2(0.003)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer = keras.regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Create a new instance of the optimizer\n",
    "adam = optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# Compile the model with the new optimizer instance\n",
    "model.compile(optimizer=adam,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(input_train, target_train,\n",
    "                 validation_data=(input_test, target_test),\n",
    "                 epochs=50,\n",
    "                 batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error, test_accuracy = model.evaluate(input_test, target_test, verbose=1)\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the overfitting is greatly reduced but still we are not able to get a good accuracy. Now we will try doing it with Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "Using keras layers of Conv2D, MaxPool2D, BatchNormalization.\n",
    "\n",
    "CNN layers takes input primarily in 3D shape, so we again have to prepare the dataset in the form and for that, I have used np.newaxis function which adds a column/layer in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(test_size, validation_size):\n",
    "    X,y = load_data(r\"./data.json\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = validation_size)\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_val = X_val[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_dataset(0.25, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1],X_train.shape[2],X_train.shape[3])\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation = \"relu\", input_shape = input_shape))\n",
    "model.add(MaxPool2D((3, 3), strides=(2, 2), padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPool2D((3, 3), strides=(2, 2), padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(32, (2, 2), activation = \"relu\"))\n",
    "model.add(MaxPool2D((2, 2), strides=(2, 2), padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(16, (1, 1), activation = \"relu\"))\n",
    "model.add(MaxPool2D((1, 1), strides=(2, 2), padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the optimizer instance with the correct argument name\n",
    "adam = optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# Compile the model with the new optimizer instance\n",
    "model.compile(optimizer=adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "hist = model.fit(X_train, y_train,\n",
    "                 validation_data = (X_val, y_val),\n",
    "                 epochs = 40,\n",
    "                 batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, y):\n",
    "    X = X[np.newaxis,...]\n",
    "    prediction = model.predict(X)\n",
    "    predicted_index = np.argmax(prediction, axis=1)\n",
    "    print(f\"Expected index: {y}, Predicted index: {predicted_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model, X_test[10], y_test[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy has improved by Significant amount but still the accuracy is not enough, in the future of this notebook I am planning to implement RNN model in this and finally use the ensembling to get a push in accuracy. However any other suggestions are always invited!"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 568973,
     "sourceId": 1032238,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30097,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
